\documentclass[11pt,a4paper]{article}

% Encoding and language
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% Graphics and layout
\usepackage{graphicx}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}

% Spacing
\linespread{1.06}
\setlength{\parskip}{8pt plus2pt minus2pt}

% Prevent widows and orphans
\widowpenalty=10000
\clubpenalty=10000

% Custom commands
\newcommand{\eat}[1]{}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% Utilities
\usepackage[official]{eurosym}
\usepackage{enumitem}
\setlist{noitemsep}
\usepackage[hidelinks]{hyperref}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{float}
\usepackage{subcaption}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}

\begin{document}
\pagenumbering{arabic} % start numbering
\setcounter{page}{1}

\begin{center}
    {\Large \bf Deep Learning in Practice - TP1 Report}\\
    {\Large \bf Hyperparameters and Training Basics with PyTorch}
    {Jean-Vincent Martini - Adonis Jamal}\\
    \today
\end{center}

\section{Multi-Classification Problem}

For this problem, we want to perform multi-class classification on the USPS dataset using a neural network implemented in PyTorch. This dataset consists of grayscale images of handwritten digits (0-9) with a resolution of 16x16 pixels. Examples of these images are shown in Figure~\ref{fig:usps_samples}.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/usps_4.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/usps_6.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/usps_9.png}
        \caption{}
    \end{subfigure}
    \caption{Samples from the USPS dataset.}
    \label{fig:usps_samples}
\end{figure}

Our neural network model consists of identical 2d convolutional layers with a kernel size of 3x3, followed by a non-linear activation function and max-pooling layers. The final layer is a fully connected layer that maps the extracted features to the 10 output classes corresponding to the digits 0-9. We chose this architecture because convolutional layers are well-suited for image data, as they can effectively capture spatial hierarchies and patterns in the images. We devided the dataset into training, validation, and test sets that we used respectively for training the model, tuning hyperparameters, and evaluating the final performance.

To tune the hyperparameters of the model, we will proceed by ablation studies, meaning that we will vary one hyperparameter at a time while keeping the others fixed to their default values. This approach allows us to isolate the effect of each hyperparameter on the model's performance and to gain time compared to a full grid search, as we will discuss later in section \ref{sec:obs}. The hyperparameters we will explore include the number of layers, the number of neurons in each hidden layer, the activation function, the batch size, the learning rate, the number of epochs, the optimizer, and the loss function. 

The default configuration of the model is as follows: 2 hidden layers with 16 neurons each, ReLU activation function, batch size of 64, learning rate of 0.001, 30 epochs, Adam optimizer, and Mean Squared Error (MSE) loss function. The results of the ablation studies for each hyperparameter are presented in the Appendix, section \ref{sec:res1}. For each configuration, we report the training accuracy, validation accuracy, and training time to provide a comprehensive view of the model's performance.

From the results, we decide to choose the final configuration of the model based on the best validation accuracy achieved for a reasonable training time. For the final configuration, we choose the same architecture as the default one, but with a learning rate of 0.01 and the loss function set to Cross-Entropy. This configuration achieves a training accuracy of 99.98\%, a validation accuracy of 98.30\%, and crucially a test accuracy of 95.81\%. The test accuracy is obtained by evaluating the final model on the test set, which was not used during training or validation in section \ref{sec:res1}. This metric therefore provides an unbiased estimate of the model's generalization performance on unseen data.

\section{Regression Problem}



\section{Observations and Trade-offs to Consider When Training Deep Learning Models}\label{sec:obs}

\newpage

\appendix
\section{Problem 1: Results for Different Hyperparameters}\label{sec:res1}

The symbol $^*$ indicates the value used in the default configuration.

\textbf{Impact of the number of layers:}

\begin{table}[H]
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        \textbf{Number of Layers} & \textbf{Train Accuracy (\%)} &
        \textbf{Validation Accuracy (\%)} & \textbf{Train Time (s)} \\
        \hline\hline
        1 & 98.57 & 96.82 & 7.91 \\
        $2^*$ & 99.08 & 97.44 & 14.52 \\
        3 & 99.22 & 97.37 & 18.26 \\
        5 & 97.83 & 96.90 & 24.69 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Impact of the number of neurons in hidden layers:}

\begin{table}[H]
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        \textbf{Hidden Layer Size} & \textbf{Train Accuracy (\%)} &
        \textbf{Validation Accuracy (\%)} & \textbf{Train Time (s)} \\
        \hline\hline
        8 & 97.72 & 96.13 & 11.62 \\
        $16^*$ & 99.08 & 97.44 & 14.52 \\
        32 & 99.68 & 98.14 & 18.94 \\
        64 & 99.90 & 98.92 & 29.29 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Impact of the activation function:}

\begin{table}[H]
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        \textbf{Activation Function} & \textbf{Train Accuracy (\%)} &
        \textbf{Validation Accuracy (\%)} & \textbf{Train Time (s)} \\
        \hline\hline
        $\text{ReLU}^*$ & 99.08 & 97.44 & 14.52 \\
        Sigmoid & 96.83 & 94.58 & 14.28 \\
        Tanh & 99.72 & 97.60 & 14.79 \\
        Leaky ReLU & 98.77 & 97.37 & 14.39 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Impact of the batch size:}

\begin{table}[H]
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        \textbf{Batch Size} & \textbf{Train Accuracy (\%)} &
        \textbf{Validation Accuracy (\%)} & \textbf{Train Time (s)} \\
        \hline\hline
        16 & 99.62 & 98.76 & 18.74 \\
        32 & 99.05 & 96.82 & 15.78 \\ 
        $64^*$ & 99.08 & 97.44 & 14.52 \\
        128 & 98.18 & 96.59 & 13.57 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Impact of the learning rate:}

\begin{table}[H]
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        \textbf{Learning Rate} & \textbf{Train Accuracy (\%)} &
        \textbf{Validation Accuracy (\%)} & \textbf{Train Time (s)} \\
        \hline\hline
        $0.001^*$ & 99.08 & 97.44 & 14.52 \\
        0.01 & 99.43 & 98.14 & 14.36 \\
        0.1 & 16.23 & 17.04 & 14.21 \\
        1 & 16.23 & 17.04 & 14.34 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Impact of the number of epochs:}

\begin{table}[H]
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        \textbf{Number of Epochs} & \textbf{Train Accuracy (\%)} &
        \textbf{Validation Accuracy (\%)} & \textbf{Train Time (s)} \\
        \hline\hline
        10 & 97.27 & 95.82 & 4.87 \\
        $30^*$ & 99.08 & 97.44 & 14.52 \\
        50 & 99.50 & 97.68 & 24.05 \\
        100 & 99.50 & 97.13 & 47.41 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Impact of the optimizer:}

\begin{table}[H]
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        \textbf{Optimizer} & \textbf{Train Accuracy (\%)} &
        \textbf{Validation Accuracy (\%)} & \textbf{Train Time (s)} \\
        \hline\hline
        $\text{Adam}^*$ & 99.08 & 97.44 & 14.52 \\
        RMSprop & 99.17 & 97.29 & 14.27 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Impact of the loss function:}

\begin{table}[H]
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        \textbf{Loss Function} & \textbf{Train Accuracy (\%)} &
        \textbf{Validation Accuracy (\%)} & \textbf{Train Time (s)} \\
        \hline\hline
        $\text{MSE}^*$ & 99.08 & 97.44 & 14.52 \\
        Cross-Entropy & 99.05 & 97.68 & 14.35 \\
        \hline
    \end{tabular}
\end{table}

\section{Problem 2: Results for Different Hyperparameters}




\end{document} 
